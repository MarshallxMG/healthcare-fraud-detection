# üéØ Healthcare Fraud Detection - PPT Questions & Answers

## üìã Project Overview Questions

### Q1: What is the main objective of your project?
**Answer:** Our project aims to detect fraudulent healthcare claims in real-time using a two-layer approach:
1. **Rule-Based Detection** - For obvious anomalies (invalid data, excessive billing)
2. **Machine Learning Model** - For detecting subtle fraud patterns

The system achieves **94.82% accuracy** and helps insurance companies and hospitals identify potentially fraudulent claims before processing.

---

### Q2: What problem does this project solve?
**Answer:** Healthcare fraud costs billions annually. Common fraud types include:
- **Upcoding** - Billing for more expensive services than provided
- **Phantom billing** - Charging for services never rendered
- **Unbundling** - Billing separately for services that should be bundled
- **Duplicate claims** - Submitting the same claim multiple times

Our system detects these patterns automatically, saving time and preventing financial losses.

---

### Q3: What is the tech stack used in this project?

| Layer | Technology |
|-------|------------|
| **Frontend** | React 18, Vite, Tailwind CSS, Recharts |
| **Backend** | FastAPI, Python 3.10+, SQLAlchemy |
| **ML** | Scikit-learn, Gradient Boosting Classifier |
| **Database** | SQLite |
| **Deployment** | Vercel (Frontend), Render (Backend) |

---

## ü§ñ Machine Learning Questions

### Q4: Why did you use Gradient Boosting instead of other algorithms?
**Answer:** We compared multiple algorithms:

| Algorithm | Why Not Chosen |
|-----------|----------------|
| Logistic Regression | Too simple for complex fraud patterns |
| Decision Tree | Overfits easily |
| SVM | Slow on large datasets |
| Neural Networks | Overkill for tabular data, less interpretable |

**Why Gradient Boosting works best:**
- Handles class imbalance (38% fraud vs 62% legitimate)
- Provides feature importance (interpretable)
- Captures non-linear patterns
- Industry standard for fraud detection

---

### Q5: What is the accuracy of your model?
**Answer:**
- **Accuracy:** 94.82%
- **ROC-AUC:** 0.9683
- **Precision (Fraud):** 91.4%
- **Recall (Fraud):** 95.8%

Cross-validation shows consistent performance: **94.14% ¬± 1.43%**

---

### Q6: What features does the ML model use?
**Answer:** The model uses **28 provider-level aggregated features**:

| Feature Category | Examples |
|------------------|----------|
| **Volume** | total_claims, unique_patients, claims_per_patient |
| **Financial** | amount_mean, amount_sum, amount_std, revenue_per_patient |
| **Complexity** | num_diagnoses_mean, num_procedures_mean |
| **Demographics** | patient_age_mean, chronic_conditions_mean |
| **Service Mix** | inpatient_ratio |

**Top 5 most important features:**
1. amount_sum (14.2%)
2. revenue_per_patient (11.8%)
3. amount_mean (9.5%)
4. claims_per_patient (8.7%)
5. total_claims (7.6%)

---

### Q7: Why did you aggregate claims at the provider level?
**Answer:** The Kaggle dataset marks **entire providers** as fraudulent, not individual claims. This means:
- A fraudulent provider has BOTH legitimate and fraudulent claims
- Training on individual claims caused confusion (only 62% accuracy)
- Training on aggregated provider statistics improved accuracy to **94.82%**

We aggregate 558,211 claims ‚Üí 5,410 providers, then train the model.

---

### Q8: How does the model make predictions on new claims?
**Answer:** Since we trained on provider aggregates, for a single new claim:
1. Extract features from the claim
2. Simulate as if it's the only claim from that provider
3. Scale features using the saved StandardScaler
4. Get probability from `model.predict_proba()`
5. Classify: Low (0-30%), Medium (30-50%), High (50-70%), Critical (70%+)

---

### Q9: What is ROC-AUC and why is it important?
**Answer:** 
- **ROC** = Receiver Operating Characteristic curve
- **AUC** = Area Under the Curve

It measures how well the model **ranks** fraud probability. 
- AUC = 1.0 ‚Üí Perfect ranking
- AUC = 0.5 ‚Üí Random guessing
- **Our AUC = 0.9683** ‚Üí Excellent ranking ability

It's better than accuracy for imbalanced datasets because it considers True Positive Rate vs False Positive Rate at all thresholds.

---

### Q10: How did you handle class imbalance?
**Answer:** Our dataset has 38% fraud and 62% legitimate. We used:
1. **Stratified splitting** - Maintains class ratio in train/test sets
2. **Class weight balancing** - Random Forest uses `class_weight='balanced'`
3. **ROC-AUC evaluation** - Better metric than accuracy for imbalanced data

---

## üîß Technical Implementation Questions

### Q11: What is the two-layer fraud detection approach?
**Answer:**
```
Layer 1: Rule-Based Detection (Instant)
‚îú‚îÄ‚îÄ Check for excessive diagnoses (>15)
‚îú‚îÄ‚îÄ Check for invalid age (<0 or >120)
‚îú‚îÄ‚îÄ Check for overpriced claims (>2.5x expected)
‚îú‚îÄ‚îÄ Check for inpatient claims with 0 stay
‚îî‚îÄ‚îÄ Check for young patients with many chronic conditions

Layer 2: ML Model (Pattern Recognition)
‚îî‚îÄ‚îÄ Gradient Boosting predicts fraud probability
```

If Layer 1 finds obvious fraud (‚â•40% probability), return immediately. Otherwise, use Layer 2.

---

### Q12: How does the disease pricing comparison work?
**Answer:** We have **6,016 diagnosis codes** with base prices. The system:

1. **Lookup base price** for the diagnosis code
2. **Apply provider multiplier:**
   - Government: 0.7x (subsidized)
   - Clinic: 1.0x (standard)
   - Private: 1.8x (premium)
3. **Calculate thresholds:**
   - Normal: ‚â§1.5x expected price
   - Elevated: 1.5x - 2.5x expected
   - Suspicious: >2.5x expected (fraud flag)

---

### Q13: What is the role of FastAPI in your project?
**Answer:** FastAPI handles:
- **/predict** - Main fraud detection endpoint
- **/stats** - Dataset statistics
- **/claims** - Recent claims list
- **/user-submissions** - Saved user entries
- **/benchmarks** - Provider pricing benchmarks

Advantages: Fast performance, automatic API docs (Swagger), type validation, async support.

---

### Q14: How is the database structured?
**Answer:** We use SQLite with SQLAlchemy ORM:

```
claims.db (Main database)
‚îú‚îÄ‚îÄ claims table (558,211 records)
‚îÇ   ‚îú‚îÄ‚îÄ claim_id, provider_id, patient_id
‚îÇ   ‚îú‚îÄ‚îÄ amount, deductible, diagnosis_code
‚îÇ   ‚îî‚îÄ‚îÄ is_fraud (target variable)

user_submissions.db (User entries)
‚îú‚îÄ‚îÄ submissions table
‚îÇ   ‚îú‚îÄ‚îÄ input data (provider, amount, etc.)
‚îÇ   ‚îî‚îÄ‚îÄ prediction results (fraud probability)
```

---

### Q15: How does the frontend communicate with the backend?
**Answer:** 
- Frontend runs on `localhost:5173` (Vite dev server)
- Backend runs on `localhost:8000` (Uvicorn)
- Communication via **REST API** with JSON payloads
- **CORS middleware** allows cross-origin requests

Example flow:
```
User clicks "Analyze" 
‚Üí React sends POST /predict 
‚Üí FastAPI processes 
‚Üí Returns JSON response 
‚Üí React displays results
```

---

## üìä Data & Dataset Questions

### Q16: What dataset did you use?
**Answer:** We used the **Kaggle Medicare Fraud Detection Dataset**:
- **558,211 claims** from Medicare beneficiaries
- **5,410 unique providers**
- Labels: Provider-level fraud indicators
- Source: CMS (Centers for Medicare & Medicaid Services) public data

---

### Q17: How did you preprocess the data?
**Answer:**
1. **Merged** inpatient and outpatient claims
2. **Created** claim_type column (Inpatient/Outpatient)
3. **Calculated** derived features (amount_per_diagnosis, length_of_stay)
4. **Joined** with fraud labels from provider dataset
5. **Filled** missing values with 0
6. **Replaced** infinity values

---

### Q18: What are ICD codes and how do you use them?
**Answer:** 
- **ICD-9/ICD-10** = International Classification of Diseases
- Standard codes for diagnoses used worldwide
- Example: Code `51881` = Acute Respiratory Failure

Our system:
1. Takes ICD code as input
2. Looks up disease name and base price
3. Uses for fraud detection and pricing comparison

---

## üåê Deployment Questions

### Q19: How is the project deployed?
**Answer:**
- **Frontend:** Deployed on Vercel (React app)
- **Backend:** Deployed on Render (FastAPI + Uvicorn)
- **Database:** SQLite file (included with deployment)
- **Model:** Pickle file (model.pkl) loaded at startup

---

### Q20: What optimizations did you implement?
**Answer:**
1. **Rate limiting** - Prevents DDoS attacks
2. **Background initialization** - DB seeding doesn't block startup
3. **Model caching** - Load model once, reuse for all requests
4. **Security headers** - XSS, CSRF protection
5. **Request logging** - Audit trail for debugging

---

## üéì Academic/Conceptual Questions

### Q21: What is Standard Scaling and why is it needed?
**Answer:** StandardScaler normalizes features to have:
- Mean = 0
- Standard Deviation = 1

Formula: `z = (x - Œº) / œÉ`

**Why needed:**
- Features have different scales (amount in millions, ratios 0-1)
- Gradient Boosting converges faster with scaled features
- Prevents features with large values from dominating

---

### Q22: What is the difference between Gradient Boosting and Random Forest?
**Answer:**

| Aspect | Gradient Boosting | Random Forest |
|--------|------------------|---------------|
| Training | Sequential (trees correct previous errors) | Parallel (independent trees) |
| Overfitting | Less prone (regularization) | More prone without tuning |
| Speed | Slower training | Faster training |
| Performance | Often better for tabular data | Good baseline |

---

### Q23: How do you evaluate fraud detection models?
**Answer:** We use multiple metrics:

| Metric | Value | Meaning |
|--------|-------|---------|
| Accuracy | 94.82% | Overall correctness |
| Precision | 91.4% | Of predicted fraud, how many are actually fraud |
| Recall | 95.8% | Of actual fraud, how many did we catch |
| ROC-AUC | 0.9683 | Ranking ability |
| F1-Score | 93.5% | Balance of precision and recall |

**Confusion Matrix interpretation:**
- True Positive: Fraud correctly caught
- False Positive: Legitimate wrongly flagged
- False Negative: Fraud missed (most costly!)
- True Negative: Legitimate correctly identified

---

### Q24: What is cross-validation and why did you use it?
**Answer:** Cross-validation:
1. Splits data into k folds (we used 5)
2. Trains on k-1 folds, tests on remaining fold
3. Repeats k times
4. Averages results

**Why used:**
- More reliable accuracy estimate
- Detects overfitting
- Uses all data for both training and testing

Our result: **94.14% ¬± 1.43%** (consistent performance)

---

### Q25: What are the limitations of your system?
**Answer:**
1. **Provider-level training** - Less accurate for new providers with few claims
2. **Static pricing** - Doesn't account for inflation or regional differences
3. **ICD-9 focus** - Limited ICD-10 support
4. **Rule thresholds** - Hardcoded values may need tuning for different contexts
5. **Single model** - Could benefit from ensemble of different models

---

## üí° Future Improvement Questions

### Q26: How would you improve this system?
**Answer:**
1. **Deep Learning** - Try neural networks for complex patterns
2. **Real-time learning** - Update model with new data
3. **Graph analysis** - Detect provider networks involved in fraud
4. **Explainable AI** - SHAP/LIME for prediction explanations
5. **Regional pricing** - Different prices for different cities
6. **ICD-10 full support** - Modern diagnosis codes

---

### Q27: How would this scale for production use?
**Answer:**
1. Replace SQLite with PostgreSQL
2. Add Redis for caching
3. Use Docker for containerization
4. Implement Kubernetes for orchestration
5. Add message queue for async processing
6. Use cloud ML services for model serving

---

## üîí Security Questions

### Q28: What security measures are implemented?
**Answer:**
1. **Rate limiting** - 100 requests/minute per IP
2. **Input validation** - Pydantic models validate all inputs
3. **CORS** - Restricts which origins can access the API
4. **Security headers** - X-Content-Type-Options, X-Frame-Options
5. **Request logging** - Audit trail for all requests
6. **Environment variables** - Sensitive configs not in code

---

## üìà Business Value Questions

### Q29: What is the business impact of this system?
**Answer:**
- **Cost savings** - Prevent fraudulent claim payments
- **Faster processing** - Automated detection vs manual review
- **Accuracy** - 94.82% accuracy reduces human error
- **Compliance** - Helps meet regulatory requirements
- **Audit trail** - All predictions logged for review

---

### Q30: Who are the target users of this system?
**Answer:**
1. **Insurance companies** - Verify claims before payment
2. **Hospitals** - Internal compliance teams
3. **Government agencies** - Medicare/Medicaid oversight
4. **Third-party administrators** - Claims processing companies
5. **Auditors** - Fraud investigation teams

---

## üé§ Common Viva Questions

### Q31: Why did you choose this project?
**Answer:** Healthcare fraud is a critical problem affecting billions in healthcare spending. This project combines:
- Real-world data (Kaggle Medicare dataset)
- Machine learning (Gradient Boosting)
- Web development (React + FastAPI)
- Database management (SQLite)

It demonstrates practical application of data science to solve a meaningful problem.

---

### Q32: What was the most challenging part?
**Answer:** 
1. **Understanding the dataset** - Provider-level vs claim-level labels
2. **Feature engineering** - Identifying meaningful aggregations
3. **Balancing accuracy vs interpretability** - Stakeholders need to understand predictions
4. **Integration** - Connecting ML model with web interface

---

### Q33: What did you learn from this project?
**Answer:**
1. End-to-end ML pipeline (data ‚Üí model ‚Üí deployment)
2. Full-stack development (React + FastAPI)
3. Domain knowledge (healthcare billing, ICD codes)
4. Model evaluation beyond accuracy
5. Production considerations (security, scaling)

---

# üìù Quick Reference

## Key Numbers to Remember
- **558,211** claims in dataset
- **5,410** unique providers
- **94.82%** model accuracy
- **0.9683** ROC-AUC score
- **28** features used
- **200** trees in Gradient Boosting
- **6,016** diagnosis codes with pricing
- **18%** GST rate

## Key Technologies
- **Frontend:** React + Vite + Tailwind
- **Backend:** FastAPI + Python
- **ML:** Scikit-learn + Gradient Boosting
- **Database:** SQLite + SQLAlchemy

## Key Concepts
- Two-layer detection (Rule-based + ML)
- Provider-level aggregation
- StandardScaler normalization
- ROC-AUC evaluation
- Cross-validation

---

**Good luck with your presentation! üéâ**
